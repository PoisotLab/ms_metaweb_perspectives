# Senior Editor Comments to Authors

This MS is strongly linked to the recently published one in MEE since it was
split from this published one early on. While the links between the 2 obvious,
the present MS should be more clearly independent from the other since the
overall perspective of the reviews is that the ms adds little to the previous
published paper.

> We would like to express our surprise regarding this comment, since the other
> article was only mentioned twice (in the caption of a figure and in a table).
> In order to address specific comment by the first reviewer, we actually had to
> cite this manuscript in a few more places. We also want to point out that the
> submitted version of this present manuscript includes (i) an overview of
> embedding techniques and their application to species interaction networks,
> (ii) a discussion of the properties of metawebs that make them amenable to
> prediction through embeddings, and (iii) a discussion of the remaining
> technical and methdological challenges associated with this approach;
> therefore, we strongly feel like the comments about this manuscript adding
> little, as all of the aforementioned elements are absent from the previous
> manuscript, are unfair.
> 
> To be perfectly transparent about our feelings reading the review, we did feel
> like our manuscript was reviewed with the mindest that it was presented as a
> Research (and not Perspective) article. We followed the MEE guidelines about
> Perspective articles, *i.e.* "to stimulate scientific debate, [by offering]
> conceptual advances or opinions or identity gaps in knowledge", which is a
> different type of work than a complete methodological treatment of the topic,
> or a narrative Review of it.
> 
> We also, notably in response to the comments made by the second reviewer, had
> to defend ourselves against accusations of sensationalism ("marketing"),
> opportunism, speculation, and general sloppyness. Although we did not assume
> that this reviewer was acting entirely in bad faith, we hope that the senior
> editor will understand that we had to take a lot of critical distance from
> these comments. We did bring specific responses to how a significant numbers
> of them were unfair, unsubstantiated, or unprofessional, but have nevertheless
> done our best to respond to them fairly; this is more than the courtesy
> extended to us by the second reviewer. It is concerning to receive reviews of
> this nature; not only do they establish the wrong example by throwing around
> unfounded accusations of low professional standards, they have a lasting
> impact on the perception of early career scientists, of which this manuscript
> has many. We do politely request that the senior editor, should this
> manuscript be sent for review again, does not asks the second reviewer for
> their advice.

In summary, they should be complementary but not overlapping to a large extent.
In the current ms, a strong message yet different from the other paper would be
that graph embedding is an emerging field in machine learning that holds great
potential for ecological problems. And this will certainly affect the whole
structure of the ms.

> We do appreciate this comment, and wholeheartedly agree. Yet, it is extremely
> difficult for us to address, as to the best of our understanding of the two
> reviews and the senior editor comments, there is not a single instance where
> areas of overlap are clearly identified, or indeed identified *at all*.
> Following the comments of reviewer 1, we do hope that the central argument
> ("graph embedding is useful for link prediction") will be clearer, but in the
> absence of specific substantiated comments about what constitutes overlap, we
> have not made significant alterations to the manuscript.

Adding a section on "how to" approach graph embedding, and a nutshell section on
embedding methods vs dimension reduction method, an example dataset to assess
with a R-code; all these would be essential for the ms even if it coud go over
the word limit. Overall, if these comments are implement together with the
suggestions of the reviewers, the current ms would be completely revamped and
would need a lot of work from the authors. yet it should produce a strong ms and
have potential to influence the field.

> We have added this section after the overview of techniques, and we now
> distribute the code to perform the analysis as Supplementary Material. This
> resulted in an additional figure, outlining a number of information that can
> be extracted from an embedding of a metaweb.

# Associate Editor Comments to Authors

(There are no comments.)

# Reviewer 1 Comments to Authors

## General comments

I read this interesting text about the idea of using embeddings to predict
metawebs. I understand that this is a perspective. About 2/3 of the manuscript
is dealing with metawebs (very pleasant to read), and the remaining 1/3
recommend the use of network embeddings (actually, the authors have mixed up
embedding methods and dimension reduction methods, which are actually two
disjoint but connected concepts).

However, I am very suprised that there is no material to support the proposal.
Do the authors already use an embedding techniques on their datasets? They claim
it is promising to use embedding with metawebs, but is it an intuition? or are
there scientifical experiments/analysis/reasoning that could support this idea?

> This is a surprising comment. The table in the original manuscript already had
> a list of embeddings methods applied to species interactions predictions,
> which in our opinion should assuage the concern of reviewer 1 by establishing
> that this is not "an intuition". Indeed, table 1 has a number of empty cases
> in the last column, which not only outlines what has been done, but what
> methods are available for application to species interactions. Furthermore, a
> part of the text discusses recent work from Strydom et al. (2022) wherein
> graph embeddings have been used to not only predict a metaweb, but do so by
> transfering information across space. Despite the fact that these information
> was already in the original manuscript, we have re-worded and expanded a
> number of sections to make them stand out more.

At some point, I am wondering if it would be have been possible to write the
same article replacing "embedding" by "graph neural networks"... These are all
interesting techniques, and I do appreciate that the authors try to help
ecologists to understand them. But we need more facts to understand the
pros/cons of these options. I am afraid the reader can not be convinced reading
the manuscript in its present form.

> This is a good question, which we explored during the writing of the initially
> submitted manuscript. GNN approaches are comparatively younger than the
> broader family of graph embeddings methods. We have added a rationale for not
> spending too much time on GNN (which would also require to introduce a solid
> amount of literature on deep learning in general), but point readers towards
> recent reviews. In order to help convince the reader, we are confident that
> the addition of an illustration or embedding on empirical data, alongside the
> discussion of already published studies, will help. Furthermore, we have
> revised the last section to bring more concrete suggestions of required work,
> which will hopefully stimulate further analyses.

I am sincerely annoyed that I can't be more positive, maybe I missed something,
and I hope that these comments can be used to improve the manuscript. But I do
think this paper is "too light" to be published in MEE.

> We hope that our gentle reminder of the nature of this (Perspective) paper as
> given in the response to the Senior Editor, will help assuage the sentiment
> that the papier is too "light"; it is not expected to be a formal comparative
> analysis of the methods, not is it expected to be an exhaustive narrative
> review. Indeed, we point out that these approaches are novel for the
> prediction of species interactions, and our hope with this manuscript is that
> more community ecologists will attempt to use them in the future. We are
> confident that the additions to the text are adressing all major comments
> raised by reviewer 1.

## Detailed comments

Until l.68: a very nice introduction about the concept of metaweb

> Thank you.

L.72-83 & Table 1 : here, there is a mixture of graph and node embeddings, i.e.
projecting a graph or a node into a low dimensional space. Would require some
clarifications.

> The second column of the table already specifies (explicitely) whether nodes
> or graphs (as well as sub-graphs, multiple graphs, and node complexes) are
> embedded given the different methods; we are not sure what the reviewer wants
> to see added as a clarification. As the information asked is already given,
> and in the absence of specific feedback, we have not changed the manuscript.

L.91 & Figure 2 : there is another subtlety here. Indeed, embedding is often
followed by a dimension reduction technique (could be PCA, t-SNE).  As in figure
2, section D is for dimension reduction. In my opinion, these are two distinct
concepts, embedding and dimension reduction. For instance, one can perform an
embedding listing a series of metrics, then perform a PCA to obtain a 2-D
representation.

> This is indeed a subtle point, and very respectfully, we think that the
> reviewer's confusion justifies the need for our manuscript. Graph embedding
> and dimensionality reduction are different yet *related* concepts. For
> example, PCA can be used initially to maximise the information in a dataset
> then fed into t-SNE; both the PCA and t-SNE step can be parameterized to
> retain fewer dimensions, but so can neither of them, with the user deciding to
> visualize a two or three dimensional representation. We have added a paragraph
> to make sure this point is extremely clear, and we hope that it will help
> readers decide on the appropriate method to use. As a sidenote (and not
> discussed in the manuscript), measuring network properties and performing a
> PCA on those (although common in network ecology) is not an embedding on the
> network, and is therefore not discussed here for the sake of text length.

l. 107 : "large confidence intervals during estimation of the values in the
low-rank space" => what does it really mean?

> We have clarified that this would hint at the fact that the two species pools
> are too distinct for transfer learning to work with adequate confidence. Thank
> you for pointing this out.

l. 109: "resulting embeddings would have interactions" => same thing. Could you
be less elusive or give references?

> We have clarified that this would lead to higher variance in the original
> dataset compared to the target dataset, leading to too much noise in the
> prediction. Thank you for pointing this out.

l. 128-141: this part is interesting at some point, but the reader can loose the
point of the article... In particular, mentioning AI here is disconnected to the
remaining text. I understand that embedding methods are part of AI at some
point, but AI gather so many domains....

> We have considerably expanded the last section, to further clarify which
> issues were "research-based" *v.* "praxis-based" problems, alongside possible
> avenues to start adressing these problems. We do hope that this will make the
> point of the last paragraph more obvious.

# Reviewer 2 Comments to Authors

## General comments

This manuscript consists in a perspective on the use of graph embeddings to
infer potential interactions, by using metadata and interaction databases. It
is, somehow, linked to the idea that potential interactions involving a new
species can be predicted from its traits (vulnerability and foraging) and
interactions involving similar species. If such an idea , based on niche
conservatism regarding interactions, is not new (e.g. Desjardins-Proulx et al.
2017, Strydom et al. 2021a), the use of graph embeddings to infer potential
interactions is a useful step forward.

> We agree with this comment; graph embeddings can indeed project networks in a
> space where the detection of unknown interactions is easier, and we support
> this claim with several examples throughout the text. No changes requested
> (none made).

However, I strongly doubt that such a short and speculative paper helps the
network ecology community.

> We have no choice bu to very firmly push back on the idea that the manuscript
> is speculative. The manuscript is prospective, or forward-looking, but every
> claim we make is either backed by facts (in the forms of references to
> existing literature in the fields of ecology, machine learning, and
> environmental governance), or accompanied (notably in the last section) by
> suggestions of additional work needed to adress some of the gaps we have
> identified. "Speculative" is an unfair assesment of our work, and we do hope
> that it did not affect the reviewer ability to take it seriously. We would
> like to point out that despite this sweeping (and serious) accusation, the
> reviewer is unable to point to a specific instance of a speculative claim.
> 
> We furthermore point out that table 1 contains examples of published research
> using the techniques we discuss, and that the text contains discussion of some
> of these papers. We are therefore confident that we clear the bar for the
> manuscript not being "speculative" by a very large margin.

Despite being appealing (it mentions the term ‘machine learning’ 10 times), I
think that it contains too much generic wishful thinking to be useful and
implementable.

> We would like to question the intent behind this comment. Is there a threshold
> above which mentioning machine learning too much opens authors to accusations
> of "generic wishful thinking"? Should a paper specifically about a field of
> machine learning *not* call the field by its name? Would a predictive method
> by any other name predict just as well?
> 
> We also would like to point out that the doubt expressed by the reviewer about
> the ability to implement these methods is directly adressed by Table 1 and the
> main text, in which we discuss examples of research papers using these exact
> methods for the prediction of species interactions. We are concerned that this
> comment, besides it inappropriate amount of sarcasm, reflects a bad faith
> reading of our manuscript.

In other words, I would expect less marketing and much more science for a paper
in Methods in Ecology and Evolution.

> We do not feel like such a cheap accusation, without any argument to back it
> up, warrants a response. Again, the reviewer is comfortable making sweeping
> statements about "marketing" without seemingly being able to identify a single
> instance that has too much marketing, or too little science.
> 
> We would expect less aggresive snark and much more constructive criticism for
> a review in Methods in Ecology and Evolution.

Moreover, the main idea of the paper (using node metadata to predict new
interactions using nodes embedding) comes from a paper of the same authors that
is not peer-reviewed and I feel a bit uncomfortable with that (see Strydom et
al. 2021b)

> This statement -- in addition to being arguably incorrect -- is no longer
> relevant as the version of the preprint we cited has now been published. We do
> hope that this brings reviewer 2 comfort.

## Detailed comments

L51: this sentence is a tautological, if interactions are represented by
parameter of bernoulli law, there are already weighted appropriately

> No changes were made - the interactions are weighed appropriately compared to
> the binary baseline, as is the point of the sentence.

L62: ‘machine learning algortihms’ : fancy term but not very specific. What
algorithms are you talking about ?

> The paper cited at the end of this sentence primarily uses artificial neural
> networks, but concludes on the potential to use "data-driven techniques",
> which feels neither less nor more specific than our reference to "ML
> algorithms"; as we do spell out in the sentence immediately after this one,
> the core point is that ML algorithms that can be made to generate data have
> the potential to augment data collected through empirical means. No changes
> made to the manuscript.

L78: Botella et al. 2022 does not use node embeddings but network embeddings. In
this case, each point of the low dimension space is a network and not a node.
Consequently, it won’t be useful to predict new potential links. Table 1 is
mixing nodes and network embeddings that do answer to different ecological
questions.

> The fact that Table 1 lists different types of embeddings is self-evident from
> the second column of table 1, which is specifically about the type of objects
> being embedded. No changes made to the manuscript.
> 
> Concerning the Botella et al. 2022 paper, the reviewer is indeed correct that
> they use graph embedding. We do agree with this interpretation. In fact,
> nowhere do we argue that their results are relevant to interaction prediction,
> and instead we summarize the findings of this article: there is a need to
> evaluate multiple embeddings for each problem, as their performance may
> differ.
> 
> All the information required was already in the manuscript, no changes were
> made.

L84: This sentence is too generic. What are machine learning techniques here ?
If you include node embedding in it, it deals with discrete data. Moreover,
there is a broad network science litterature that deals with discrete data.

> The techniques we discuss are listed in the reference given immediately at the
> end of the sentence. There is indeed literature on discrete data (we do not
> make claims to the contrary); our point remains that projecting discrete data
> into a dense, continuous space comes with a list of benefits (that are listed
> in the remainder of this paragraph, together with references to the relevant
> literature). No changes were made.

L129: This sounds a bit superficial and opportunistic. I don't think such a
serious topic can be adequately covered in a short paragraph. Yes, we all know
that machine learning techniques will save the world, thanks.

> We would like to question whether this dismisiveness of our work is
> contributing in a positive way to the discussion that is supposed to take
> place as part of the review process, and whether this is the behavior the
> second reviewer thinks should be demonstrated to early career authors.
> 
> To be candid, the comments by reviewer 2 mostly come across as opposed to our
> work as a matter of principle, but (more concerningly) as refusing to take it
> seriously, by chosing to expres a concerning number of points through sarcasm
> rather than in a professional voice more becoming of the review process.
> 
> We did our best in our response to the specific points below to not
> reciprocate, and explained why changes were (or were not) made, but we cannot
> weigh this review equally to the constructive comments made by the first
> reviewer. "Superficial", but mostly "opportunistic", are egregious attacks on
> our profesionalism, and insinuate that we did not write the manuscript in good
> faith. They do not deserve a reply, for they should never have been written.

More seriously, we should not forget that these techniques are built by people
and not machines. Consequently, they are also subjects to cultural/national
biases since influencing academic centers are located in the western world.

> We do agree with the reviewer, and this is, in fact, a core point of the
> paragraph, and of most of the articles cited therein. As the reviewer and
> authors are in agreement, no changes were made.

L135: I doubt that ‘algorithmic thinking’ is generating new knowledge thanks to
machine learning. Actually, ecological knowledge is generated by people, not
machines. Of course, statistical techniques help scientists to analyze and
structure data. But they must remain tools in the hands of individuals.

> We do agree with the reviewer (but would like to point out that the definition
> of algorithmic thinking we use comes from the article cited immediately before
> we first mention "algorithmic thinking"). We have added a reference to
> "algorithm in the loop" approaches (in other fields, as this notion has been
> under-explored in ecological sciences), alongside their possible issues, and
> suggestions as to what the next steps are.

L140: But it should also be recognized that these tools, because they can be
difficult to understand for non-specialists, can pose problems of
interpretation. Moreover, they can make those who make decisions dependent on a
tool that they do not master.

> This has been adressed as part of the previous comment, by emphasizing the
> need for training and computational literacy in decision makers.

L145 : Long acknowledgements section for a short paper !

> We refuse to engage with this comment.